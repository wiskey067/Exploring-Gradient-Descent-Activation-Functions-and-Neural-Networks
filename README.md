Neural Network Optimization & Activation Function Analysis


Author: Arijit Bhattacharjee


ğŸ§  Overview

This project explores core components of neural networks through hands-on experimentation and analysis using the sklearn digits dataset. Key focus areas include:
	â€¢	Custom implementation of gradient descent
	â€¢	Comparison of sigmoid, tanh, and ReLU activation functions
	â€¢	Numerical vs. analytical gradient checking
	â€¢	Neural network training using different optimizers (SGD, Adam, LBFGS)
	â€¢	Performance comparison in terms of accuracy, loss convergence, and training time

â¸»

ğŸ“ Table of Contents
	1.	Introduction
	2.	Experimental Setup
	3.	Visualizations
	4.	Results Summary
	5.	Key Findings
	6.	Conclusion
	7.	Appendix

â¸»

ğŸ”¬ Introduction

This project investigates how optimization techniques and activation functions affect neural network performance for a binary classification task. We classify handwritten digits from the sklearn.datasets.load_digits() dataset, relabeled as:
	â€¢	Class 0: Digits 0â€“4
	â€¢	Class 1: Digits 5â€“9

The project includes:
	â€¢	Custom implementation of gradient descent
	â€¢	Analytical and numerical gradient verification
	â€¢	Model training using MLPClassifier with various solvers
	â€¢	Visual comparison of activation functions and optimizers

â¸»

âš™ï¸ Experimental Setup
	â€¢	Dataset: sklearn.datasets.load_digits()
	â€¢	Binary Labels: 0â€“4 â†’ 0, and 5â€“9 â†’ 1
	â€¢	Preprocessing: Standardization and 80-20 Train/Test Split
	â€¢	Libraries Used: NumPy, Matplotlib, scikit-learn
	â€¢	Network Type: Feedforward MLP
	â€¢	Solvers Tested: SGD, Adam, LBFGS
	â€¢	Custom Modules:
	â€¢	Gradient Descent
	â€¢	Activation functions and derivatives
	â€¢	Gradient checking (numerical vs. backprop)

â¸»

ğŸ“Š Visualizations

ğŸ” Activation Functions & Their Gradients
	â€¢	Functions: Sigmoid, Tanh, ReLU
	â€¢	Observation:
	â€¢	ReLU maintains strong gradients
	â€¢	Sigmoid and Tanh suffer from vanishing gradients in deeper layers

ğŸ“‰ Loss Curves (Adam vs. SGD)
	â€¢	Adam converges faster and more consistently
	â€¢	SGD shows slower convergence with more fluctuations

ğŸ¯ Accuracy and Time Comparison
	â€¢	Adam and LBFGS achieve high accuracy in less time
	â€¢	SGD is computationally slower but stable

â¸»

ğŸ“‹ Results Summary

Optimizer	Train Accuracy	Test Accuracy	Training Time
SGD	~97%	~94%	High
Adam	~99%	~96%	Medium
LBFGS	~99%	~95%	Lowest

(Exact values may vary slightly with random seed)

â¸»

ğŸ“Œ Key Findings

Optimizer Performance
	â€¢	SGD: Stable but slow; good for small-scale training
	â€¢	Adam: Best balance of speed and accuracy
	â€¢	LBFGS: Fastest but not ideal for large datasets

Activation Functions
	â€¢	ReLU performed best, especially in deeper networks
	â€¢	Sigmoid/Tanh were slower and susceptible to vanishing gradients

General Observations
	â€¢	Overfitting Risk: Slight overfitting observed in LBFGS
	â€¢	Gradient Checking: Analytical gradients matched numerical ones
	â€¢	Performance Bottlenecks: Manual gradient computation is slow; use auto-diff frameworks for scaling

â¸»

âœ… Conclusion
	â€¢	Best Combo: Adam + ReLU for this classification task
	â€¢	For Larger Networks: Monitor for overfitting, especially with fast optimizers
	â€¢	Custom Code Insights: Great for educational use; inefficient for large-scale production training

â¸»

ğŸ“ Appendix
	â€¢	Notebook: gradient_descent_neural_net.ipynb
	â€¢	Libraries Used:
	â€¢	numpy
	â€¢	matplotlib
	â€¢	sklearn
	â€¢	All plots, tables, and logs included in the notebook



ğŸ“Œ How to Run

# Clone the repo
git clone https://github.com/wiskey067/gradient-descent-nn-experiments.git
cd gradient-descent-nn-experiments

# Launch Jupyter
jupyter notebook gradient_descent_neural_net.ipynb




ğŸ™Œ Acknowledgements

Thanks to the scikit-learn and numpy communities for open-source contributions that made this analysis possible.
